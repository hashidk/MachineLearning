{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35121f41",
   "metadata": {},
   "source": [
    "# 4.5. Weight Decay\n",
    " COLAB [MXNET]Open the notebook in Colab\n",
    " SAGEMAKER STUDIO LABOpen the notebook in SageMaker Studio Lab\n",
    "Now that we have characterized the problem of overfitting, we can introduce some standard techniques for regularizing models. Recall that we can always mitigate overfitting by going out and collecting more training data. That can be costly, time consuming, or entirely out of our control, making it impossible in the short run. For now, we can assume that we already have as much high-quality data as our resources permit and focus on regularization techniques.\n",
    "\n",
    "Recall that in our polynomial regression example (Section 4.4) we could limit our model’s capacity simply by tweaking the degree of the fitted polynomial. Indeed, limiting the number of features is a popular technique to mitigate overfitting. However, simply tossing aside features can be too blunt an instrument for the job. Sticking with the polynomial regression example, consider what might happen with high-dimensional inputs. The natural extensions of polynomials to multivariate data are called monomials, which are simply products of powers of variables. The degree of a monomial is the sum of the powers. For example, \n",
    ", and \n",
    " are both monomials of degree 3.\n",
    "\n",
    "Note that the number of terms with degree \n",
    " blows up rapidly as \n",
    " grows larger. Given \n",
    " variables, the number of monomials of degree \n",
    " (i.e., \n",
    " multichoose \n",
    ") is \n",
    ". Even small changes in degree, say from \n",
    " to \n",
    ", dramatically increase the complexity of our model. Thus we often need a more fine-grained tool for adjusting function complexity.\n",
    "\n",
    "## 4.5.1. Norms and Weight Decay\n",
    "We have described both the \n",
    " norm and the \n",
    " norm, which are special cases of the more general \n",
    " norm in Section 2.3.10. Weight decay (commonly called \n",
    " regularization), might be the most widely-used technique for regularizing parametric machine learning models. The technique is motivated by the basic intuition that among all functions \n",
    ", the function \n",
    " (assigning the value \n",
    " to all inputs) is in some sense the simplest, and that we can measure the complexity of a function by its distance from zero. But how precisely should we measure the distance between a function and zero? There is no single right answer. In fact, entire branches of mathematics, including parts of functional analysis and the theory of Banach spaces, are devoted to answering this issue.\n",
    "\n",
    "One simple interpretation might be to measure the complexity of a linear function \n",
    " by some norm of its weight vector, e.g., \n",
    ". The most common method for ensuring a small weight vector is to add its norm as a penalty term to the problem of minimizing the loss. Thus we replace our original objective, minimizing the prediction loss on the training labels, with new objective, minimizing the sum of the prediction loss and the penalty term. Now, if our weight vector grows too large, our learning algorithm might focus on minimizing the weight norm \n",
    " vs. minimizing the training error. That is exactly what we want. To illustrate things in code, let us revive our previous example from Section 3.1 for linear regression. There, our loss was given by\n",
    "\n",
    "(4.5.1)\n",
    " \n",
    " \n",
    " \n",
    "Recall that \n",
    " are the features, \n",
    " are labels for all data examples \n",
    ", and \n",
    " are the weight and bias parameters, respectively. To penalize the size of the weight vector, we must somehow add \n",
    " to the loss function, but how should the model trade off the standard loss for this new additive penalty? In practice, we characterize this tradeoff via the regularization constant \n",
    ", a non-negative hyperparameter that we fit using validation data:\n",
    "\n",
    "(4.5.2)\n",
    " \n",
    "For \n",
    ", we recover our original loss function. For \n",
    ", we restrict the size of \n",
    ". We divide by \n",
    " by convention: when we take the derivative of a quadratic function, the \n",
    " and \n",
    " cancel out, ensuring that the expression for the update looks nice and simple. The astute reader might wonder why we work with the squared norm and not the standard norm (i.e., the Euclidean distance). We do this for computational convenience. By squaring the \n",
    " norm, we remove the square root, leaving the sum of squares of each component of the weight vector. This makes the derivative of the penalty easy to compute: the sum of derivatives equals the derivative of the sum.\n",
    "\n",
    "Moreover, you might ask why we work with the \n",
    " norm in the first place and not, say, the \n",
    " norm. In fact, other choices are valid and popular throughout statistics. While \n",
    "-regularized linear models constitute the classic ridge regression algorithm, \n",
    "-regularized linear regression is a similarly fundamental model in statistics, which is popularly known as lasso regression.\n",
    "\n",
    "One reason to work with the \n",
    " norm is that it places an outsize penalty on large components of the weight vector. This biases our learning algorithm towards models that distribute weight evenly across a larger number of features. In practice, this might make them more robust to measurement error in a single variable. By contrast, \n",
    " penalties lead to models that concentrate weights on a small set of features by clearing the other weights to zero. This is called feature selection, which may be desirable for other reasons.\n",
    "\n",
    "Using the same notation in (3.1.10), the minibatch stochastic gradient descent updates for \n",
    "-regularized regression follow:\n",
    "\n",
    "(4.5.3)\n",
    " \n",
    " \n",
    " \n",
    "As before, we update \n",
    " based on the amount by which our estimate differs from the observation. However, we also shrink the size of \n",
    " towards zero. That is why the method is sometimes called “weight decay”: given the penalty term alone, our optimization algorithm decays the weight at each step of training. In contrast to feature selection, weight decay offers us a continuous mechanism for adjusting the complexity of a function. Smaller values of \n",
    " correspond to less constrained \n",
    ", whereas larger values of \n",
    " constrain \n",
    " more considerably.\n",
    "\n",
    "Whether we include a corresponding bias penalty \n",
    " can vary across implementations, and may vary across layers of a neural network. Often, we do not regularize the bias term of a network’s output layer.\n",
    "\n",
    "## 4.5.2. High-Dimensional Linear Regression\n",
    "We can illustrate the benefits of weight decay through a simple synthetic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d39015",
   "metadata": {},
   "outputs": [],
   "source": [
    "use strict;\n",
    "use warnings;\n",
    "use Data::Dump qw(dump);\n",
    "use AI::MXNet qw(mx);\n",
    "use AI::MXNet::Gluon qw(gluon);\n",
    "use List::Util qw(min max shuffle);\n",
    "use d2l;\n",
    "use d2l::Accumulator;\n",
    "use d2l::Animator;\n",
    "IPerl->load_plugin('Chart::Plotly');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781b03b",
   "metadata": {},
   "source": [
    "First, we generate some data as before\n",
    "\n",
    "(4.5.4)\n",
    " \n",
    "We choose our label to be a linear function of our inputs, corrupted by Gaussian noise with zero mean and standard deviation 0.01. To make the effects of overfitting pronounced, we can increase the dimensionality of our problem to \n",
    " and work with a small training set containing only 20 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d622d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub load_array{\n",
    "    my ($features, $labels, $batch_size, $shuffle, $seed) = @_;\n",
    "    srand ($seed) if defined $seed;\n",
    "    my $num_samples = $features->len;\n",
    "    my @indices = $shuffle ? shuffle (0 .. $num_samples - 1) : (0 .. $num_samples - 1);\n",
    "    my ($index, @batch_indices) = 0;\n",
    "    $features = $features->asarray;\n",
    "    $labels = $labels->asarray;\n",
    "    return sub {\n",
    "        if (defined $_[0] && $_[0] == 0) {\n",
    "        @indices = shuffle @indices if $shuffle;\n",
    "        $index = 0;\n",
    "        return 1;\n",
    "     }\n",
    "     return undef if ($index >= $num_samples);\n",
    "     @batch_indices = @indices[$index .. min($index + $batch_size, $num_samples) - 1];\n",
    "     $index += $batch_size;\n",
    "     return {data => mx->nd->array([@$features [@batch_indices]]),\n",
    "             label => mx->nd->array([@$labels [@batch_indices]])};\n",
    "  };\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7893ceb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CODE(0xb124c08)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my ($n_train, $n_test, $batch_size) = (20, 100, 5);\n",
    "our $num_inputs = 200;\n",
    "my ($true_w, $true_b) = (mx->nd->ones([$num_inputs, 1]) * 0.01, 0.05);\n",
    "our ($train_data, $labels) = d2l->synthetic_data($true_w, $true_b, $n_train);\n",
    "our $train_iter = load_array($train_data, $labels, $batch_size, 0);\n",
    "my ($test_data, $test_labels)  = d2l->synthetic_data($true_w, $true_b, $n_test);\n",
    "our $test_iter = load_array($test_data, $test_labels, $batch_size, is_train => 1 );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a9459",
   "metadata": {},
   "source": [
    "## 4.5.3. Implementation from Scratch\n",
    "In the following, we will implement weight decay from scratch, simply by adding the squared \n",
    " penalty to the original target function.\n",
    "\n",
    "### 4.5.3.1. Initializing Model Parameters\n",
    "First, we will define a function to randomly initialize our model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96b6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub init_params{\n",
    "    our $num_inputs;\n",
    "    my $w = mx->nd->random->normal(scale => 1, shape => [$num_inputs, 1]);\n",
    "    my $b = mx->nd->zeros([1]);\n",
    "    $w->attach_grad();\n",
    "    $b->attach_grad();\n",
    "    return ($w, $b);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf36ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AI::MXNet::NDArray 200x1 @cpu(0)><AI::MXNet::NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_params();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e4464",
   "metadata": {},
   "source": [
    "### 4.5.3.2. Defining  Norm Penalty\n",
    "Perhaps the most convenient way to implement this penalty is to square all terms in place and sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0b5a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub l2_penalty{\n",
    "    my $w = shift;\n",
    "    return (($w->sum()) / 2);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df03368",
   "metadata": {},
   "source": [
    "### 4.5.3.3. Defining the Training Loop\n",
    "The following code fits a model on the training set and evaluates it on the test set. The linear network and the squared loss have not changed since Section 3, so we will just import them via d2l.linreg and d2l.squared_loss. The only change here is that our loss now includes the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02db6e4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "Subroutine train redefined at reply input line 1.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine train redefined at reply input line 1.\n"
     ]
    }
   ],
   "source": [
    "sub train{\n",
    "    my $lambd = @_;\n",
    "    my ($w, $b) = init_params();\n",
    "    my $net = \\&{'linreg'};\n",
    "    my $loss = \\&{'squared_loss'};\n",
    "    my ($num_epochs, $lr) = (100, 0.003);\n",
    "  #  my $animator = Animator();\n",
    "    our ($train_iter, $test_iter);\n",
    "    our ($train_data, $labels);\n",
    "    for my $epoch (1 .. $num_epochs) {\n",
    "        while (my $minibatch = $train_iter->()){\n",
    "            my $X = $minibatch->{data};\n",
    "            my $y = $minibatch->{label};\n",
    "            my $l;\n",
    "            autograd->record(sub {\n",
    "                $l = (($loss->($net->($X), $y))+ $lambd * l2_penalty($w));\n",
    "             });\n",
    "            $l->backward();\n",
    "           # if (($epoch + 1) % 5 == 0) {\n",
    "             #   $animator->add(($epoch + 1), (d2l->evaluate_loss($net, $train_iter, $loss),\n",
    "             #                                   d2l->evaluate_loss($net, $test_iter, $loss)));\n",
    "            #    }\n",
    "            \n",
    "            }\n",
    "        }\n",
    "        my $traces->[0] = Chart::Plotly::Trace::Scatter->new(name => \"True Regression Line\",\n",
    "                                                   x => $train_data->asarray,\n",
    "                                                   y => $labels->asarray,\n",
    "                                                   mode => \"lines\",\n",
    "                                                   line => {color => 'blue',\n",
    "                                                   width => 3});\n",
    "    print('L2 norm of w:', dump mx->nd->norm($w)->asarray);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57936e",
   "metadata": {},
   "source": [
    "### 4.5.3.4. Training without Regularization\n",
    "We now run this code with lambd = 0, disabling weight decay. Note that we overfit badly, decreasing the training error but not the test error—a textbook case of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d705100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w:[14.2511596679688]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82227236",
   "metadata": {},
   "source": [
    "### 4.5.3.5. Using Weight Decay\n",
    "Below, we run with substantial weight decay. Note that the training error increases but the test error decreases. This is precisely the effect we expect from regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ee1f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w:[14.513542175293]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(3);  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851eae6d",
   "metadata": {},
   "source": [
    "## 4.5.4. Concise Implementation\n",
    "Because weight decay is ubiquitous in neural network optimization, the deep learning framework makes it especially convenient, integrating weight decay into the optimization algorithm itself for easy use in combination with any loss function. Moreover, this integration serves a computational benefit, allowing implementation tricks to add weight decay to the algorithm, without any additional computational overhead. Since the weight decay portion of the update depends only on the current value of each parameter, the optimizer must touch each parameter once anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04f019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub train_concise{\n",
    "    my $wd = shift;\n",
    "    my $net = nn->Sequential();\n",
    "    $net->add(nn->Dense(1));\n",
    "    $net->initialize(mx->init->Normal(sigma => 1));\n",
    "    my $loss = gluon->loss->L2Loss();\n",
    "    my ($num_epochs, $lr) = (100, 0.003);\n",
    "    my $trainer = gluon->Trainer($net->collect_params(),\n",
    "                                optimizer => 'sgd',\n",
    "                                optimizer_params => { learning_rate => $lr, wd=> $wd});\n",
    "    # The bias parameter has not decayed. Bias names generally end with \"bias\"\n",
    "    $net->collect_params('.*bias')->setattr('wd_mult', 0);\n",
    "    #y $animator = Animator(xlabel => 'epochs', ylabel => 'loss', yscale => 'log',\n",
    "     #                      xlim => [5, $num_epochs], legend => ['train', 'test']);\n",
    "    our ($train_iter, $test_iter);\n",
    "    our ($train_data, $labels);\n",
    "    \n",
    "    for my $epoch (1 .. $num_epochs) {\n",
    "        while (my $minibatch = $train_iter->()){\n",
    "            my $X = $minibatch->{data};\n",
    "            my $y = $minibatch->{label};\n",
    "            my $l;\n",
    "            autograd->record(sub {\n",
    "                $l = $loss->($net->($X), $y);\n",
    "             });\n",
    "            $l->backward();\n",
    "           # if (($epoch + 1) % 5 == 0) {\n",
    "             #   $animator->add(($epoch + 1), (d2l->evaluate_loss($net, $train_iter, $loss),\n",
    "             #                                   d2l->evaluate_loss($net, $test_iter, $loss)));\n",
    "            #    }\n",
    "            \n",
    "            }\n",
    "        }\n",
    "    my $traces->[0] = Chart::Plotly::Trace::Scatter->new(name => \"True Regression Line\",\n",
    "                                                   x => $train_data->asarray,\n",
    "                                                   y => $labels->asarray,\n",
    "                                                   mode => \"lines\",\n",
    "                                                   line => {color => 'blue',\n",
    "                                                   width => 3});\n",
    "\n",
    "    print('L2 norm of w:', mx->nd->norm($net->weight->data()->at(0)));  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77978fd",
   "metadata": {},
   "source": [
    "The plots look identical to those when we implemented weight decay from scratch. However, they run appreciably faster and are easier to implement, a benefit that will become more pronounced for larger problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6d1a61",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Can't call method \"data\" on an undefined value at reply input line 41.\n",
     "output_type": "error",
     "traceback": [
      "Can't call method \"data\" on an undefined value at reply input line 41.\n"
     ]
    }
   ],
   "source": [
    "train_concise(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb1b81",
   "metadata": {},
   "source": [
    "So far, we only touched upon one notion of what constitutes a simple linear function. Moreover, what constitutes a simple nonlinear function can be an even more complex question. For instance, reproducing kernel Hilbert space (RKHS) allows one to apply tools introduced for linear functions in a nonlinear context. Unfortunately, RKHS-based algorithms tend to scale poorly to large, high-dimensional data. In this book we will default to the simple heuristic of applying weight decay on all layers of a deep network.\n",
    "\n",
    "## 4.5.5. Summary\n",
    "Regularization is a common method for dealing with overfitting. It adds a penalty term to the loss function on the training set to reduce the complexity of the learned model.\n",
    "\n",
    "One particular choice for keeping the model simple is weight decay using an  penalty. This leads to weight decay in the update steps of the learning algorithm.\n",
    "\n",
    "The weight decay functionality is provided in optimizers from deep learning frameworks.\n",
    "\n",
    "Different sets of parameters can have different update behaviors within the same training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881c856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
