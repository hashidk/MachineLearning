{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96747fd",
   "metadata": {},
   "source": [
    "# 4.4.4 Polynomial Regression\n",
    "We can now explore these concepts interactively by fitting polynomials to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8f653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import math\n",
    "##from mxnet import gluon, np, npx\n",
    "##from mxnet.gluon import nn\n",
    "##from d2l import mxnet as d2l\n",
    "##npx.set_np()\n",
    "\n",
    "use strict;\n",
    "use warnings;\n",
    "use Data::Dump qw(dump);\n",
    "use AI::MXNet qw(mx);\n",
    "use AI::MXNet::Gluon qw(gluon);\n",
    "use List::Util qw(min max shuffle);\n",
    "use d2l;\n",
    "use d2l::Accumulator;\n",
    "use d2l::Animator;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a548e6",
   "metadata": {},
   "source": [
    "### Generating the Dataset\n",
    "First we need data. Given x, we will use the following cubic polynomial to generate the labels on\n",
    "training and test data:\n",
    "\n",
    "$$ y = 5 + 1.2x − 3.4\\frac{x^2}{2!} + 5.6\\frac{x^3}{3!} + ϵ $$\n",
    "$$ where: ϵ ∼ \\mathcal{N} (0, {0.1}^{2}) $$\n",
    "The noise term ϵ obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. For\n",
    "optimization, we typically want to avoid very large values of gradients or losses. This is why the\n",
    "features are rescaled from xi to xii!. It allows us to avoid very large values for large exponents i. We\n",
    "will synthesize 100 samples each for the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1683297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub gamma {\n",
    "    ##source code: (https://hewgill.com/picomath/perl/gamma.pl.html)\n",
    "    my $x = $_[0];\n",
    "\n",
    "    if ($x <= 0.0){\n",
    "        die \"Invalid input argument $x. Argument must be positive\";\n",
    "    }\n",
    "\n",
    "    my $gamma = 0.577215664901532860606512090; # Euler's gamma constant\n",
    "\n",
    "    if ($x < 0.001) {\n",
    "        return 1.0/($x*(1.0 + $gamma*$x));\n",
    "    }\n",
    "\n",
    "    if ($x < 12.0)\n",
    "    {\n",
    "        my $y = $x;\n",
    "        my $n = 0;\n",
    "        my $arg_was_less_than_one = ($y < 1.0);\n",
    "\n",
    "        if ($arg_was_less_than_one){\n",
    "            $y += 1.0;\n",
    "        }else{\n",
    "            $n = int($y) - 1;  # will use n later\n",
    "            $y -= $n;\n",
    "        }\n",
    "\n",
    "        my @p =\n",
    "        (\n",
    "            -1.71618513886549492533811E+0,\n",
    "             2.47656508055759199108314E+1,\n",
    "            -3.79804256470945635097577E+2,\n",
    "             6.29331155312818442661052E+2,\n",
    "             8.66966202790413211295064E+2,\n",
    "            -3.14512729688483675254357E+4,\n",
    "            -3.61444134186911729807069E+4,\n",
    "             6.64561438202405440627855E+4\n",
    "        );\n",
    "        my @q =\n",
    "        (\n",
    "            -3.08402300119738975254353E+1,\n",
    "             3.15350626979604161529144E+2,\n",
    "            -1.01515636749021914166146E+3,\n",
    "            -3.10777167157231109440444E+3,\n",
    "             2.25381184209801510330112E+4,\n",
    "             4.75584627752788110767815E+3,\n",
    "            -1.34659959864969306392456E+5,\n",
    "            -1.15132259675553483497211E+5\n",
    "        );\n",
    "\n",
    "        my $num = 0.0;\n",
    "        my $den = 1.0;\n",
    "        my $i;\n",
    "\n",
    "        my $z = $y - 1;\n",
    "        for ($i = 0; $i < 8; $i++){\n",
    "            $num = ($num + $p[$i])*$z;\n",
    "            $den = $den*$z + $q[$i];\n",
    "        }\n",
    "        my $result = $num/$den + 1.0;\n",
    "\n",
    "        if ($arg_was_less_than_one){\n",
    "            $result /= ($y-1.0);\n",
    "        }else{\n",
    "            for ($i = 0; $i < $n; $i++) {\n",
    "                $result *= $y++;\n",
    "            }\n",
    "        }\n",
    "        return $result;\n",
    "    }\n",
    "\n",
    "    if ($x > 171.624){\n",
    "        return undef;\n",
    "    }\n",
    "\n",
    "    return exp(log_gamma($x));\n",
    "}\n",
    "\n",
    "sub log_gamma {\n",
    "    my $x = $_[0];\n",
    "\n",
    "    if ($x <= 0.0)\n",
    "    {\n",
    "        die \"Invalid input argument $x. Argument must be positive\";\n",
    "    }\n",
    "\n",
    "    if ($x < 12.0)\n",
    "    {\n",
    "        return log(abs(gamma($x)));\n",
    "    }\n",
    "\n",
    "    # Abramowitz and Stegun 6.1.41\n",
    "    # Asymptotic series should be good to at least 11 or 12 figures\n",
    "    # For error analysis, see Whittiker and Watson\n",
    "    # A Course in Modern Analysis (1927), page 252\n",
    "\n",
    "    my @c =\n",
    "    (\n",
    "         1.0/12.0,\n",
    "        -1.0/360.0,\n",
    "         1.0/1260.0,\n",
    "        -1.0/1680.0,\n",
    "         1.0/1188.0,\n",
    "        -691.0/360360.0,\n",
    "         1.0/156.0,\n",
    "        -3617.0/122400.0\n",
    "    );\n",
    "    my $z = 1.0/($x*$x);\n",
    "    my $sum = $c[7];\n",
    "    for (my $i=6; $i >= 0; $i--)\n",
    "    {\n",
    "        $sum *= $z;\n",
    "        $sum += $c[$i];\n",
    "    }\n",
    "    my $series = $sum/$x;\n",
    "\n",
    "    my $halfLogTwoPi = 0.91893853320467274178032973640562;\n",
    "    my $logGamma = ($x - 0.5)*log($x) - $x + $halfLogTwoPi + $series;    \n",
    "    return $logGamma;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35a5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub getSubND{\n",
    "    my ($input) = @_;\n",
    "       if(!$input->{data}){\n",
    "        print \"ERR: No ha colocado el array\";\n",
    "        return 1;\n",
    "    }\n",
    "    if(!$input->{row_end}){\n",
    "        print \"ERR: No ha colocado el valor de row_end\";\n",
    "        return 1;\n",
    "    }else{\n",
    "        if(!$input->{row_start}){\n",
    "            $input->{row_start} = 0;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if($input->{column_end} && $input->{data}->shape->[1]){\n",
    "        if(!$input->{column_start}){\n",
    "            $input->{column_start} = 0;\n",
    "        }    \n",
    "        my @arr;\n",
    "        for my $i ($input->{row_start}..$input->{row_end}-1){\n",
    "            push(@arr, $input->{data}->[$i]->_slice($input->{column_start}, $input->{column_end})->asarray);\n",
    "        }\n",
    "        return mx->nd->array(\\@arr);\n",
    "    }else{\n",
    "        if(!$input->{data}->shape->[1]){\n",
    "            return $input->{data}->_slice($input->{row_start}, $input->{row_end});\n",
    "        }else{\n",
    "            print \"ERR: No ha colocado el valor de column_end\";\n",
    "            return 1;        \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d6d3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AI::MXNet::NDArray 200 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $max_degree = 20; ## Maximum degree of the polynomial\n",
    "my ($n_train, $n_test) = (100, 100); ## Training and test dataset sizes\n",
    "my $true_w = mx->nd->zeros([$max_degree]); ## Allocate lots of empty space\n",
    "$true_w = $true_w->asarray;\n",
    "@$true_w[0..3] = (5, 1.2, -3.4, 5.6);\n",
    "$true_w = mx->nd->array($true_w);\n",
    "\n",
    "my $features = mx->nd->random->normal(shape=>[$n_train + $n_test, 1]);\n",
    "$features = mx->nd->shuffle($features);\n",
    "\n",
    "my $poly_features = $features ** mx->nd->arange(stop=>$max_degree)->reshape([1,-1]);\n",
    "\n",
    "my $len = $poly_features->shape->[0];\n",
    "$poly_features = $poly_features->asarray;\n",
    "for my $i (0..$max_degree-1){\n",
    "    my $gamma = gamma($i + 1);\n",
    "    for my $j (0..$len-1){\n",
    "        $$poly_features[$j][$i] = $$poly_features[$j][$i] / $gamma;\n",
    "    }\n",
    "}\n",
    "$poly_features = mx->nd->array($poly_features);\n",
    "\n",
    "my $labels = mx->nd->dot($poly_features, $true_w);\n",
    "$labels += mx->nd->random->normal(scale=>0.1, shape=>$labels->shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd864dee",
   "metadata": {},
   "source": [
    "Again, monomials stored in poly_features are rescaled by the gamma function, where Γ(n) =\n",
    "(n − 1)!. Take a look at the first 2 samples from the generated dataset. The value 1 is technically a\n",
    "feature, namely the constant feature corresponding to the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca0d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.50947511196136], [1.96766126155853]]\n",
      "[\n",
      "  [\n",
      "    1,\n",
      "    1.50947511196136,\n",
      "    1.13925755023956,\n",
      "    0.573226988315582,\n",
      "    0.216317966580391,\n",
      "    0.0653053149580956,\n",
      "    0.0164294578135014,\n",
      "    0.00354283698834479,\n",
      "    0.000668478023726493,\n",
      "    0.000112116766104009,\n",
      "    1.6923748262343e-05,\n",
      "    2.32236129704688e-06,\n",
      "    2.921289024016e-07,\n",
      "    3.39200987298227e-08,\n",
      "    3.65725338902223e-09,\n",
      "    3.68035518727439e-10,\n",
      "    3.47212814055808e-11,\n",
      "    3.08299458011418e-12,\n",
      "    2.58539066891328e-13,\n",
      "    2.05399120021404e-14,\n",
      "  ],\n",
      "  [\n",
      "    1,\n",
      "    1.96766126155853,\n",
      "    1.93584537506104,\n",
      "    1.26969599723816,\n",
      "    0.624582946300507,\n",
      "    0.245793521404266,\n",
      "    0.0806063935160637,\n",
      "    0.0226580128073692,\n",
      "    0.00557291181758046,\n",
      "    0.00121840031351894,\n",
      "    0.000239739893004298,\n",
      "    4.28842649853323e-05,\n",
      "    7.03180876371334e-06,\n",
      "    1.06432446500548e-06,\n",
      "    1.49587862097178e-07,\n",
      "    1.96225489190738e-08,\n",
      "    2.41315811777554e-09,\n",
      "    2.79310463646709e-10,\n",
      "    3.05326874894263e-11,\n",
      "    3.1619990913806e-12,\n",
      "  ],\n",
      "]\n",
      "[6.18047618865967, 7.75959348678589]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##print dump features[:2], poly_features[:2, :], labels[:2]\n",
    "print dump(getSubND({data=>$features, row_start=>0, row_end=>2, column_end=>1})->asarray);\n",
    "print \"\\n\";\n",
    "print dump((getSubND({data=>$poly_features, row_start=>0, column_start=>0, row_end=>2, column_end=>$poly_features->shape->[1]}))->asarray);\n",
    "print \"\\n\";\n",
    "print dump((getSubND({data=>$labels, row_start=>0, row_end=>2}))->asarray);\n",
    "print \"\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1489cf1",
   "metadata": {},
   "source": [
    "### Training and Testing the Model\n",
    "Let us first implement a function to evaluate the loss on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad2db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub evaluate_loss{\n",
    "    my ($net, $data_iter, $loss) = @_;\n",
    "    #\"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
    "    my $metric = Accumulator->new(2); # Sum of losses, no. of examples\n",
    "    my ($X, $y, $accuracy, $l);\n",
    "    while (defined(my $batch = $data_iter->())){\n",
    "        $X = $batch->{data};\n",
    "        $y = $batch->{label}->astype('float32');\n",
    "        \n",
    "        $l = $loss->($net->($X), $y);\n",
    "        $metric->add([ $l->sum(), $l->size]);\n",
    "    }\n",
    "    if($metric->getitem(1)==0){\n",
    "        return (0);\n",
    "    }else{\n",
    "        return ($metric->getitem(0) / $metric->getitem(1));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52739a8b",
   "metadata": {},
   "source": [
    "Now define the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa142d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub train_epoch_ch3{ #@save OPTIONAL - TESTING\n",
    "    my ($net, $train_iter, $loss, $updater) = @_;\n",
    "    my $metric = Accumulator->new(3);\n",
    "    $updater = $updater->step(10,1);\n",
    "\n",
    "    \n",
    "    while (defined(my $batch = $train_iter->())){\n",
    "        our $X = $batch->{data};\n",
    "        our $y = $batch->{label}->astype('float32');\n",
    "    \n",
    "        our $y_hat;\n",
    "        our $l;\n",
    "        mx->autograd->record(sub {\n",
    "            $y_hat = $net->($X);\n",
    "            $l = $loss->($y_hat, $y);\n",
    "        });\n",
    "        $l->backward;\n",
    "        $updater->step($X->shape->[0], 1);\n",
    "        \n",
    "        $metric->add([$l->sum()->astype('float32'), accuracy($y_hat, $y), $y->size])\n",
    "    }\n",
    "    \n",
    "    if($metric->getitem(2)==0){\n",
    "        return (0);\n",
    "    }else{\n",
    "        return ($metric->getitem(0) / $metric->getitem(2), $metric->getitem(1) / $metric->getitem(2));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdcec7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub data_iter_sequential{ # Optimized for sequential minibatches\n",
    "  my ($features, $labels, $batch_size) = @_;\n",
    "  my $num_samples = $features->len;\n",
    "  my @indices = (0 .. $num_samples - 1);\n",
    "  my ($index, @batch_indices) = 0;\n",
    "\n",
    "  return sub {\n",
    "    if (defined $_[0] && $_[0] == 0){# Reset\n",
    "      $index = 0;\n",
    "      return 1;\n",
    "    }\n",
    "    return undef if ($index >= $num_samples);\n",
    "    @batch_indices = @indices[$index .. min($index + $batch_size, $num_samples) - 1];\n",
    "    $index += $batch_size;\n",
    "    return {data  => $features->slice([$batch_indices[0], $batch_indices[-1]]), \n",
    "            label => $labels->slice([$batch_indices[0], $batch_indices[-1]])};\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb973c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub train{\n",
    "    my ($train_features, $test_features, $train_labels, $test_labels, $num_epochs) = @_;\n",
    "    if(!$num_epochs) {$num_epochs = 2;}\n",
    "\n",
    "    my $batch_size = min(10, $train_labels->shape->[0]);\n",
    "    my $loss = gluon->loss->L2Loss();\n",
    "    my $net = gluon->nn->Sequential();\n",
    "    $net->name_scope(sub {\n",
    "        $net->add(gluon->nn->Dense(1, use_bias=>0, in_units=>4))\n",
    "        });\n",
    "    # Switch off the bias since we already catered for it in the polynomial features\n",
    "    $net->initialize();\n",
    "    \n",
    "    ##my $train_iter = d2l->load_array([$train_features, $train_labels], $batch_size);\n",
    "    my $train_iter = data_iter_sequential($train_features, $train_labels, $batch_size);\n",
    "    ##my $test_iter = d2l->load_array([$test_features, $test_labels], $batch_size);#, is_train=>0);\n",
    "    my $test_iter = data_iter_sequential($test_features, $test_labels, $batch_size);\n",
    "    ##print dump $train_iter->();\n",
    "    my $trainer = gluon->Trainer($net->collect_params(), 'sgd', {learning_rate => 0.01});\n",
    "\n",
    "    my $animator = Animator->new(xlabel=>'epoch', ylabel=>'loss', yscale=>'logarithm', xlim=>[1, $num_epochs], ylim=>[1e-3, 1e2], legend=>['train', 'test']);\n",
    "    \n",
    "    for my $epoch (0..$num_epochs-1){\n",
    "        print $epoch . \"\\n\";\n",
    "        train_epoch_ch3($net, $train_iter, $loss, $trainer);        \n",
    "        if( $epoch == 0 || ($epoch + 1) % 20 == 0){\n",
    "            $animator->add($epoch + 1, (evaluate_loss($net, $train_iter, $loss), evaluate_loss($net, $test_iter, $loss)))\n",
    "        }         \n",
    "    }\n",
    "\n",
    "    #print('weight:', $net->[0]->weight->data()->asnumpy())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae784ad0",
   "metadata": {},
   "source": [
    "### Third-Order Polynomial Function Fitting (Normal)\n",
    "We will begin by first using a third-order polynomial function, which is the same order as that\n",
    "of the data generation function. The results show that this modelʼs training and test losses can\n",
    "be both effectively reduced. The learned model parameters are also close to the true values w =\n",
    "[5, 1.2, −3.4, 5.6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57250a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the\n",
    "# polynomial features\n",
    "train(\n",
    "    getSubND({data=>$poly_features, row_start=>0, column_start=>0, row_end=>$n_train, column_end=>4}),\n",
    "    getSubND({data=>$poly_features, row_start=>$n_train+1, column_start=>0, row_end=>$poly_features->shape->[0], column_end=>4}),\n",
    "    getSubND({data=>$labels, row_start=>0, row_end=>$n_train}),\n",
    "    getSubND({data=>$labels, row_start=>$n_train+1, row_end=>$labels->shape->[0]})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b772dd",
   "metadata": {},
   "source": [
    "### Linear Function Fitting (Underfitting)\n",
    "Let us take another look at linear function fitting. After the decline in early epochs, it becomes\n",
    "difficult to further decrease this modelʼs training loss. After the last epoch iteration has been\n",
    "completed, the training loss is still high. When used to fit nonlinear patterns (like the third-order\n",
    "polynomial function here) linear models are liable to underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first two dimensions, i.e., 1, x, from the polynomial features\n",
    "train(\n",
    "    getSubND({data=>$poly_features, row_start=>0, column_start=>0, row_end=>$n_train, column_end=>2}),\n",
    "    getSubND({data=>$poly_features, row_start=>$n_train+1, column_start=>0, row_end=>$poly_features->shape->[0], column_end=>2}),\n",
    "    getSubND({data=>$labels, row_start=>0, row_end=>$n_train}),\n",
    "    getSubND({data=>$labels, row_start=>$n_train+1, row_end=>$labels->shape->[0]})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e24b4",
   "metadata": {},
   "source": [
    "### Higher-Order Polynomial Function Fitting (Overfitting)\n",
    "Now let us try to train the model using a polynomial of too high degree. Here, there are insufficient\n",
    "data to learn that the higher-degree coefficients should have values close to zero. As a result, our\n",
    "overly-complex model is so susceptible that it is being influenced by noise in the training data.\n",
    "Though the training loss can be effectively reduced, the test loss is still much higher. It shows that\n",
    "the complex model overfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick all the dimensions from the polynomial features\n",
    "train(\n",
    "    getSubND({data=>$poly_features, row_start=>0, column_start=>0, row_end=>$n_train, column_end=>$poly_features->shape->[1]}),\n",
    "    getSubND({data=>$poly_features, row_start=>$n_train+1, column_start=>0, row_end=>$poly_features->shape->[0], column_end=>$poly_features->shape->[1]}),\n",
    "    getSubND({data=>$labels, row_start=>0, row_end=>$n_train}),\n",
    "    getSubND({data=>$labels, row_start=>$n_train+1, row_end=>$labels->shape->[0]}),\n",
    "    1500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bb25b",
   "metadata": {},
   "source": [
    "In the subsequent sections, we will continue to discuss overfitting problems and methods for dealing with them, such as weight decay and dropout.\n",
    "\n",
    "### Summary\n",
    "* Since the generalization error cannot be estimated based on the training error, simply minimizing the training error will not necessarily mean a reduction in the generalization error. <br> Machine learning models need to be careful to safeguard against overfitting so as to minimize the generalization error.\n",
    "* A validation set can be used for model selection, provided that it is not used too liberally.\n",
    "* Underfitting means that a model is not able to reduce the training error. When training error is much lower than validation error, there is overfitting.\n",
    "* We should choose an appropriately complex model and avoid using insufficient training samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
