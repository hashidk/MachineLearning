{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e96747fd",
   "metadata": {},
   "source": [
    "# 4.4.4 Polynomial Regression\n",
    "We can now explore these concepts interactively by fitting polynomials to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "from d2l import mxnet as d2l\n",
    "npx.set_np()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95a548e6",
   "metadata": {},
   "source": [
    "### Generating the Dataset\n",
    "First we need data. Given x, we will use the following cubic polynomial to generate the labels on\n",
    "training and test data:\n",
    "\n",
    "$$ y = 5 + 1.2x − 3.4\\frac{x^2}{2!} + 5.6\\frac{x^3}{3!} + ϵ $$\n",
    "$$ where: ϵ ∼ \\mathcal{N} (0, {0.1}^{2}) $$\n",
    "The noise term ϵ obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. For\n",
    "optimization, we typically want to avoid very large values of gradients or losses. This is why the\n",
    "features are rescaled from xi to xii!. It allows us to avoid very large values for large exponents i. We\n",
    "will synthesize 100 samples each for the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba250ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 20 # Maximum degree of the polynomial\n",
    "n_train, n_test = 100, 100 # Training and test dataset sizes\n",
    "true_w = np.zeros(max_degree) # Allocate lots of empty space\n",
    "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
    "\n",
    "features = np.random.normal(size=(n_train + n_test, 1))\n",
    "np.random.shuffle(features)\n",
    "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
    "for i in range(max_degree):\n",
    "    poly_features[:, i] /= math.gamma(i + 1) # `gamma(n)` = (n-1)!\n",
    "# Shape of `labels`: (`n_train` + `n_test`,)\n",
    "labels = np.dot(poly_features, true_w)\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd864dee",
   "metadata": {},
   "source": [
    "Again, monomials stored in poly_features are rescaled by the gamma function, where Γ(n) =\n",
    "(n − 1)!. Take a look at the first 2 samples from the generated dataset. The value 1 is technically a\n",
    "feature, namely the constant feature corresponding to the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:2], poly_features[:2, :], labels[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1489cf1",
   "metadata": {},
   "source": [
    "### Training and Testing the Model\n",
    "Let us first implement a function to evaluate the loss on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(net, data_iter, loss): #@save\n",
    "    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
    "    metric = d2l.Accumulator(2) # Sum of losses, no. of examples\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y)\n",
    "        metric.add(l.sum(), l.size)\n",
    "    return metric[0] / metric[1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52739a8b",
   "metadata": {},
   "source": [
    "Now define the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb973c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_features, test_features, train_labels, test_labels, num_epochs=400):\n",
    "    loss = gluon.loss.L2Loss()\n",
    "    net = nn.Sequential()\n",
    "    # Switch off the bias since we already catered for it in the polynomial\n",
    "    # features\n",
    "    net.add(nn.Dense(1, use_bias=False))\n",
    "    net.initialize()\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_labels), batch_size, is_train=False)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log', xlim=[1, num_epochs], ylim=[1e-3, 1e2], legend=['train', 'test'])\n",
    "    for epoch in range(num_epochs):\n",
    "        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss), evaluate_loss(net, test_iter, loss)))\n",
    "    print('weight:', net[0].weight.data().asnumpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae784ad0",
   "metadata": {},
   "source": [
    "### Third-Order Polynomial Function Fitting (Normal)\n",
    "We will begin by first using a third-order polynomial function, which is the same order as that\n",
    "of the data generation function. The results show that this modelʼs training and test losses can\n",
    "be both effectively reduced. The learned model parameters are also close to the true values w =\n",
    "[5, 1.2, −3.4, 5.6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57250a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the\n",
    "# polynomial features\n",
    "train(poly_features[:n_train, :4], poly_features[n_train:, :4], labels[:n_train], labels[n_train:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61b772dd",
   "metadata": {},
   "source": [
    "### Linear Function Fitting (Underfitting)\n",
    "Let us take another look at linear function fitting. After the decline in early epochs, it becomes\n",
    "difficult to further decrease this modelʼs training loss. After the last epoch iteration has been\n",
    "completed, the training loss is still high. When used to fit nonlinear patterns (like the third-order\n",
    "polynomial function here) linear models are liable to underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first two dimensions, i.e., 1, x, from the polynomial features\n",
    "train(poly_features[:n_train, :2], poly_features[n_train:, :2], labels[:n_train], labels[n_train:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e55e24b4",
   "metadata": {},
   "source": [
    "### Higher-Order Polynomial Function Fitting (Overfitting)\n",
    "Now let us try to train the model using a polynomial of too high degree. Here, there are insufficient\n",
    "data to learn that the higher-degree coefficients should have values close to zero. As a result, our\n",
    "overly-complex model is so susceptible that it is being influenced by noise in the training data.\n",
    "Though the training loss can be effectively reduced, the test loss is still much higher. It shows that\n",
    "the complex model overfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick all the dimensions from the polynomial features\n",
    "train(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:], num_epochs=1500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f98bb25b",
   "metadata": {},
   "source": [
    "In the subsequent sections, we will continue to discuss overfitting problems and methods for dealing with them, such as weight decay and dropout.\n",
    "\n",
    "### Summary\n",
    "* Since the generalization error cannot be estimated based on the training error, simply minimizing the training error will not necessarily mean a reduction in the generalization error. <br> Machine learning models need to be careful to safeguard against overfitting so as to minimize the generalization error.\n",
    "* A validation set can be used for model selection, provided that it is not used too liberally.\n",
    "* Underfitting means that a model is not able to reduce the training error. When training error is much lower than validation error, there is overfitting.\n",
    "* We should choose an appropriately complex model and avoid using insufficient training samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
