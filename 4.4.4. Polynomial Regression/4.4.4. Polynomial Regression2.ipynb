{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96747fd",
   "metadata": {},
   "source": [
    "# 4.4.4 Polynomial Regression\n",
    "We can now explore these concepts interactively by fitting polynomials to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e8f653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import math\n",
    "##from mxnet import gluon, np, npx\n",
    "##from mxnet.gluon import nn\n",
    "##from d2l import mxnet as d2l\n",
    "##npx.set_np()\n",
    "\n",
    "use strict;\n",
    "use warnings;\n",
    "use Data::Dump qw(dump);\n",
    "use AI::MXNet qw(mx);\n",
    "use AI::MXNet::Gluon qw(gluon);\n",
    "use List::Util qw(min max shuffle);\n",
    "use d2l;\n",
    "use d2l::Accumulator;\n",
    "use d2l::Animator;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a548e6",
   "metadata": {},
   "source": [
    "### Generating the Dataset\n",
    "First we need data. Given x, we will use the following cubic polynomial to generate the labels on\n",
    "training and test data:\n",
    "\n",
    "$$ y = 5 + 1.2x − 3.4\\frac{x^2}{2!} + 5.6\\frac{x^3}{3!} + ϵ $$\n",
    "$$ where: ϵ ∼ \\mathcal{N} (0, {0.1}^{2}) $$\n",
    "The noise term ϵ obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. For\n",
    "optimization, we typically want to avoid very large values of gradients or losses. This is why the\n",
    "features are rescaled from xi to xii!. It allows us to avoid very large values for large exponents i. We\n",
    "will synthesize 100 samples each for the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1683297a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "Subroutine gamma redefined at reply input line 1.\n\nSubroutine log_gamma redefined at reply input line 79.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine gamma redefined at reply input line 1.\n\nSubroutine log_gamma redefined at reply input line 79.\n"
     ]
    }
   ],
   "source": [
    "sub gamma {\n",
    "    ##source code: (https://hewgill.com/picomath/perl/gamma.pl.html)\n",
    "    my $x = $_[0];\n",
    "\n",
    "    if ($x <= 0.0){\n",
    "        die \"Invalid input argument $x. Argument must be positive\";\n",
    "    }\n",
    "\n",
    "    my $gamma = 0.577215664901532860606512090; # Euler's gamma constant\n",
    "\n",
    "    if ($x < 0.001) {\n",
    "        return 1.0/($x*(1.0 + $gamma*$x));\n",
    "    }\n",
    "\n",
    "    if ($x < 12.0)\n",
    "    {\n",
    "        my $y = $x;\n",
    "        my $n = 0;\n",
    "        my $arg_was_less_than_one = ($y < 1.0);\n",
    "\n",
    "        if ($arg_was_less_than_one){\n",
    "            $y += 1.0;\n",
    "        }else{\n",
    "            $n = int($y) - 1;  # will use n later\n",
    "            $y -= $n;\n",
    "        }\n",
    "\n",
    "        my @p =\n",
    "        (\n",
    "            -1.71618513886549492533811E+0,\n",
    "             2.47656508055759199108314E+1,\n",
    "            -3.79804256470945635097577E+2,\n",
    "             6.29331155312818442661052E+2,\n",
    "             8.66966202790413211295064E+2,\n",
    "            -3.14512729688483675254357E+4,\n",
    "            -3.61444134186911729807069E+4,\n",
    "             6.64561438202405440627855E+4\n",
    "        );\n",
    "        my @q =\n",
    "        (\n",
    "            -3.08402300119738975254353E+1,\n",
    "             3.15350626979604161529144E+2,\n",
    "            -1.01515636749021914166146E+3,\n",
    "            -3.10777167157231109440444E+3,\n",
    "             2.25381184209801510330112E+4,\n",
    "             4.75584627752788110767815E+3,\n",
    "            -1.34659959864969306392456E+5,\n",
    "            -1.15132259675553483497211E+5\n",
    "        );\n",
    "\n",
    "        my $num = 0.0;\n",
    "        my $den = 1.0;\n",
    "        my $i;\n",
    "\n",
    "        my $z = $y - 1;\n",
    "        for ($i = 0; $i < 8; $i++){\n",
    "            $num = ($num + $p[$i])*$z;\n",
    "            $den = $den*$z + $q[$i];\n",
    "        }\n",
    "        my $result = $num/$den + 1.0;\n",
    "\n",
    "        if ($arg_was_less_than_one){\n",
    "            $result /= ($y-1.0);\n",
    "        }else{\n",
    "            for ($i = 0; $i < $n; $i++) {\n",
    "                $result *= $y++;\n",
    "            }\n",
    "        }\n",
    "        return $result;\n",
    "    }\n",
    "\n",
    "    if ($x > 171.624){\n",
    "        return undef;\n",
    "    }\n",
    "\n",
    "    return exp(log_gamma($x));\n",
    "}\n",
    "\n",
    "sub log_gamma {\n",
    "    my $x = $_[0];\n",
    "\n",
    "    if ($x <= 0.0)\n",
    "    {\n",
    "        die \"Invalid input argument $x. Argument must be positive\";\n",
    "    }\n",
    "\n",
    "    if ($x < 12.0)\n",
    "    {\n",
    "        return log(abs(gamma($x)));\n",
    "    }\n",
    "\n",
    "    # Abramowitz and Stegun 6.1.41\n",
    "    # Asymptotic series should be good to at least 11 or 12 figures\n",
    "    # For error analysis, see Whittiker and Watson\n",
    "    # A Course in Modern Analysis (1927), page 252\n",
    "\n",
    "    my @c =\n",
    "    (\n",
    "         1.0/12.0,\n",
    "        -1.0/360.0,\n",
    "         1.0/1260.0,\n",
    "        -1.0/1680.0,\n",
    "         1.0/1188.0,\n",
    "        -691.0/360360.0,\n",
    "         1.0/156.0,\n",
    "        -3617.0/122400.0\n",
    "    );\n",
    "    my $z = 1.0/($x*$x);\n",
    "    my $sum = $c[7];\n",
    "    for (my $i=6; $i >= 0; $i--)\n",
    "    {\n",
    "        $sum *= $z;\n",
    "        $sum += $c[$i];\n",
    "    }\n",
    "    my $series = $sum/$x;\n",
    "\n",
    "    my $halfLogTwoPi = 0.91893853320467274178032973640562;\n",
    "    my $logGamma = ($x - 0.5)*log($x) - $x + $halfLogTwoPi + $series;    \n",
    "    return $logGamma;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e35a5f7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "Subroutine getSubND redefined at reply input line 2.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine getSubND redefined at reply input line 2.\n"
     ]
    }
   ],
   "source": [
    "## Simular el slice, ejem: arr[0:8, 0:8]\n",
    "sub getSubND{\n",
    "    my ($input) = @_;\n",
    "       if(!$input->{data}){\n",
    "        print \"ERR: No ha colocado el array\";\n",
    "        return 1;\n",
    "    }\n",
    "    if(!$input->{row_end}){\n",
    "        print \"ERR: No ha colocado el valor de row_end\";\n",
    "        return 1;\n",
    "    }else{\n",
    "        if(!$input->{row_start}){\n",
    "            $input->{row_start} = 0;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if($input->{column_end} && $input->{data}->shape->[1]){\n",
    "        if(!$input->{column_start}){\n",
    "            $input->{column_start} = 0;\n",
    "        }    \n",
    "        my @arr;\n",
    "        for my $i ($input->{row_start}..$input->{row_end}-1){\n",
    "            push(@arr, $input->{data}->[$i]->_slice($input->{column_start}, $input->{column_end})->asarray);\n",
    "        }\n",
    "        return mx->nd->array(\\@arr);\n",
    "    }else{\n",
    "        if(!$input->{data}->shape->[1]){\n",
    "            return $input->{data}->_slice($input->{row_start}, $input->{row_end});\n",
    "        }else{\n",
    "            print \"ERR: No ha colocado el valor de column_end\";\n",
    "            return 1;        \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0d6d3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AI::MXNet::NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $max_degree = 5; ## Maximum degree of the polynomial\n",
    "my $n = 10;\n",
    "my ($n_train, $n_test) = ($n, $n); ## Training and test dataset sizes\n",
    "my $true_w = mx->nd->zeros([$max_degree]); ## Allocate lots of empty space\n",
    "$true_w = $true_w->asarray;\n",
    "@$true_w[0..3] = (5, 1.2, -3.4, 5.6);\n",
    "$true_w = mx->nd->array($true_w);\n",
    "\n",
    "my $features = mx->nd->random->normal(shape=>[$n_train + $n_test, 1]);\n",
    "$features = mx->nd->shuffle($features);\n",
    "\n",
    "my $poly_features = $features ** mx->nd->arange(stop=>$max_degree)->reshape([1,-1]);\n",
    "\n",
    "my $len = $poly_features->shape->[0];\n",
    "$poly_features = $poly_features->asarray;\n",
    "for my $i (0..$max_degree-1){\n",
    "    my $gamma = gamma($i + 1);\n",
    "    for my $j (0..$len-1){\n",
    "        $$poly_features[$j][$i] = $$poly_features[$j][$i] / $gamma;\n",
    "    }\n",
    "}\n",
    "$poly_features = mx->nd->array($poly_features);\n",
    "\n",
    "my $labels = mx->nd->dot($poly_features, $true_w);\n",
    "$labels += mx->nd->random->normal(scale=>0.1, shape=>$labels->shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd864dee",
   "metadata": {},
   "source": [
    "Again, monomials stored in poly_features are rescaled by the gamma function, where Γ(n) =\n",
    "(n − 1)!. Take a look at the first 2 samples from the generated dataset. The value 1 is technically a\n",
    "feature, namely the constant feature corresponding to the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bca0d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.891265451908112], [3.2904531955719]]\n",
      "[\n",
      "  [\n",
      "    1,\n",
      "    0.891265451908112,\n",
      "    0.397177040576935,\n",
      "    0.117996729910374,\n",
      "    0.0262916013598442,\n",
      "  ],\n",
      "  [\n",
      "    1,\n",
      "    3.2904531955719,\n",
      "    5.41354131698608,\n",
      "    5.93766784667969,\n",
      "    4.88440465927124,\n",
      "  ],\n",
      "]\n",
      "[5.48033285140991, 23.7382698059082]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##print dump features[:2], poly_features[:2, :], labels[:2]\n",
    "print dump(getSubND({data=>$features, row_start=>0, row_end=>2, column_end=>1})->asarray);\n",
    "print \"\\n\";\n",
    "print dump((getSubND({data=>$poly_features, row_start=>0, column_start=>0, row_end=>2, column_end=>$poly_features->shape->[1]}))->asarray);\n",
    "print \"\\n\";\n",
    "print dump((getSubND({data=>$labels, row_start=>0, row_end=>2}))->asarray);\n",
    "print \"\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1489cf1",
   "metadata": {},
   "source": [
    "### Training and Testing the Model\n",
    "Let us first implement a function to evaluate the loss on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aad2db49",
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "Subroutine evaluate_loss redefined at reply input line 1.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine evaluate_loss redefined at reply input line 1.\n"
     ]
    }
   ],
   "source": [
    "sub evaluate_loss{\n",
    "    my ($net, $data_iter, $loss) = @_;\n",
    "    #\"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
    "    my $metric = Accumulator->new(2); # Sum of losses, no. of examples\n",
    "    while (defined(my $batch = <$data_iter>)){\n",
    "        my $X = $batch->{data};\n",
    "        my $y = $batch->{label}->astype('float32');\n",
    "        \n",
    "        my $l = $loss->($net->($X), $y);\n",
    "        $metric->add([ $l->sum->asscalar, $l->size]);\n",
    "    }\n",
    "    if($metric->getitem(1)==0){\n",
    "        return (0);\n",
    "    }else{\n",
    "        return ($metric->getitem(0) / $metric->getitem(1));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52739a8b",
   "metadata": {},
   "source": [
    "Now define the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84b25115",
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "Subroutine load_array redefined at reply input line 1.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine load_array redefined at reply input line 1.\n"
     ]
    }
   ],
   "source": [
    "sub load_array{\n",
    "    my ($X, $y, $batch_size, $is_train) = @_;\n",
    "    my $dataset = gluon->data->ArrayDataset(data=>$X, label=>$y);\n",
    "    return gluon->data->DataLoader($dataset, batch_size=> $batch_size, shuffle=>$is_train);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd884a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "Subroutine train_epoch_ch3 redefined at reply input line 1.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine train_epoch_ch3 redefined at reply input line 1.\n"
     ]
    }
   ],
   "source": [
    "sub train_epoch_ch3{ #@save\n",
    "  #Train a model within one epoch (defined in Chapter 3).\n",
    "  #Sum of training loss, sum of training accuracy, no. of examples\n",
    "  my ($net, $train_iter, $loss, $updater, $batch_size) = @_;\n",
    "  my $metric = Accumulator->new(3);\n",
    "  \n",
    "  if (ref($updater) eq 'AI::MXNet::Gluon::Trainer'){\n",
    "    $updater->step( $batch_size, 1);\n",
    "  }  \n",
    "  my ($X, $y, $y_hat, $l);\n",
    "  \n",
    "  while(defined(my $batch = <$train_iter>)){\n",
    "    # Compute gradients and update parameters\n",
    "    $X = $batch->[0];\n",
    "    $y = $batch->[1]->astype('float32'); \n",
    "    autograd->record(sub {\n",
    "      $y_hat = $net->($X);\n",
    "      $l = $loss->($y_hat, $y);\n",
    "    });\n",
    "    $l->backward();\n",
    "\n",
    "    if (ref($updater) eq 'AI::MXNet::Gluon::Trainer'){\n",
    "      $updater->step( $batch_size);\n",
    "    }else{\n",
    "      $updater->($batch_size);\n",
    "    }\n",
    "\n",
    "    $metric->add([ $l->sum->asscalar, accuracy($y_hat, $y), $y->size ]);\n",
    "  }\n",
    "  # Return training loss and training accuracy\n",
    "  return ($metric->getitem(0) / $metric->getitem(2), $metric->getitem(1) / $metric->getitem(2));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fb973c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "Subroutine train redefined at reply input line 1.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine train redefined at reply input line 1.\n"
     ]
    }
   ],
   "source": [
    "sub train{\n",
    "    my ($train_features, $test_features, $train_labels, $test_labels, $num_epochs) = @_;\n",
    "    if(!$num_epochs) {$num_epochs = 2;}\n",
    "\n",
    "    my $batch_size = min(10, $train_labels->shape->[0]);\n",
    "    my $loss = gluon->loss->L2Loss();\n",
    "    my $net = gluon->nn->Sequential();\n",
    "    $net->name_scope(sub {\n",
    "        $net->add(gluon->nn->Dense(1, use_bias=>0, in_units=>$train_features->shape->[1]))\n",
    "        });\n",
    "    # Switch off the bias since we already catered for it in the polynomial features\n",
    "    $net->initialize();\n",
    "    \n",
    "    my $train_iter = load_array($train_features, $train_labels, $batch_size, 1);\n",
    "    my $test_iter = load_array($test_features, $test_labels, $batch_size, 0);\n",
    "\n",
    "    my $trainer = gluon->Trainer($net->collect_params(), 'sgd', {learning_rate => 0.01});\n",
    "\n",
    "    my $animator = Animator->new(xlabel=>'epoch', ylabel=>'loss', yscale=>'logarithm', xlim=>[1, $num_epochs], ylim=>[1e-3, 1e2], legend=>['train', 'test']);\n",
    "    \n",
    "    for my $epoch (0..$num_epochs-1){\n",
    "        print $epoch . \"\\n\";\n",
    "        train_epoch_ch3($net, $train_iter, $loss, $trainer, $batch_size);        \n",
    "        if( $epoch == 0 || ($epoch + 1) % 20 == 0){\n",
    "            $animator->add($epoch + 1, (evaluate_loss($net, $train_iter, $loss), evaluate_loss($net, $test_iter, $loss)))\n",
    "        }      \n",
    "        last;\n",
    "    }\n",
    "\n",
    "    print('weight:', $net->[0]->weight->data()->asnumpy())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae784ad0",
   "metadata": {},
   "source": [
    "### Third-Order Polynomial Function Fitting (Normal)\n",
    "We will begin by first using a third-order polynomial function, which is the same order as that\n",
    "of the data generation function. The results show that this modelʼs training and test losses can\n",
    "be both effectively reduced. The learned model parameters are also close to the true values w =\n",
    "[5, 1.2, −3.4, 5.6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57250a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the\n",
    "# polynomial features\n",
    "train(\n",
    "    getSubND({data=>$poly_features, row_start=>0, column_start=>0, row_end=>$n_train, column_end=>4}),\n",
    "    getSubND({data=>$poly_features, row_start=>$n_train+1, column_start=>0, row_end=>$poly_features->shape->[0], column_end=>4}),\n",
    "    getSubND({data=>$labels, row_start=>0, row_end=>$n_train}),\n",
    "    getSubND({data=>$labels, row_start=>$n_train+1, row_end=>$labels->shape->[0]})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b772dd",
   "metadata": {},
   "source": [
    "### Linear Function Fitting (Underfitting)\n",
    "Let us take another look at linear function fitting. After the decline in early epochs, it becomes\n",
    "difficult to further decrease this modelʼs training loss. After the last epoch iteration has been\n",
    "completed, the training loss is still high. When used to fit nonlinear patterns (like the third-order\n",
    "polynomial function here) linear models are liable to underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cec7c1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Undefined subroutine &main::net called at reply input line 16.\n at /usr/local/lib/perl5/site_perl/5.32.1/AI/MXNet/AutoGrad.pm line 408.\n\tAI::MXNet::AutoGrad::record(\"autograd\", CODE(0xb89ae50)) called at reply input line 18\n\tmain::train_epoch_ch3(CODE(0xb9a84e0), AI::MXNet::Gluon::Data::Loader::DataLoader=HASH(0xb99f6d0), AI::MXNet::Gluon::L2Loss=HASH(0xb9964f0), REF(0xba0d718)) called at reply input line 23\n\tmain::train(AI::MXNet::NDArray=HASH(0xb886910), AI::MXNet::NDArray=HASH(0xb9f7af0), AI::MXNet::NDArray=HASH(0x948d680), AI::MXNet::NDArray=HASH(0xb995c68)) called at reply input line 2\n\tEval::Closure::Sandbox_1288::__ANON__() called at /usr/local/lib/perl5/site_perl/5.32.1/Reply/Plugin/Defaults.pm line 71\n\tReply::Plugin::Defaults::execute(Reply::Plugin::Defaults=HASH(0x4ffc8b0), CODE(0xb886460), CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 217\n\tReply::_wrapped_plugin(Reply=HASH(0x5084558), ARRAY(0x572d7e0), \"execute\", CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 215\n\tReply::__ANON__(CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply/Plugin/IPerl.pm line 28\n\tReply::Plugin::IPerl::__ANON__() called at /usr/local/lib/perl5/site_perl/5.32.1/Capture/Tiny.pm line 382\n\teval {...} called at /usr/local/lib/perl5/site_perl/5.32.1/Capture/Tiny.pm line 382\n\tCapture::Tiny::_capture_tee(1, 1, 0, 0, CODE(0x2b15468)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply/Plugin/IPerl.pm line 29\n\tReply::Plugin::IPerl::execute(Reply::Plugin::IPerl=HASH(0x50b0fb0), CODE(0x572d468), CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 217\n\tReply::_wrapped_plugin(Reply=HASH(0x5084558), \"execute\", CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 174\n\tReply::_eval(Reply=HASH(0x5084558), \"\\x{a}#line 1 \\\"reply input\\\"\\x{a}# Pick the first two dimensions, i.e.,\"...) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 66\n\tReply::try {...} () called at /usr/local/lib/perl5/site_perl/5.32.1/Try/Tiny.pm line 102\n\teval {...} called at /usr/local/lib/perl5/site_perl/5.32.1/Try/Tiny.pm line 93\n\tTry::Tiny::try(CODE(0x57189c8), Try::Tiny::Catch=REF(0xb988cd0)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 71\n\tReply::step(Reply=HASH(0x5084558), \"# Pick the first two dimensions, i.e., 1, x, from the polynom\"..., 0) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel/Backend/Reply.pm line 48\n\tDevel::IPerl::Kernel::Backend::Reply::__ANON__() called at /usr/local/lib/perl5/site_perl/5.32.1/Capture/Tiny.pm line 382\n\teval {...} called at /usr/local/lib/perl5/site_perl/5.32.1/Capture/Tiny.pm line 382\n\tCapture::Tiny::_capture_tee(1, 1, 0, 0, CODE(0xb86ead8)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel/Backend/Reply.pm line 49\n\tDevel::IPerl::Kernel::Backend::Reply::run_line(Devel::IPerl::Kernel::Backend::Reply=HASH(0x29aab28), \"# Pick the first two dimensions, i.e., 1, x, from the polynom\"...) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel/Callback/REPL.pm line 42\n\tDevel::IPerl::Kernel::Callback::REPL::execute(Devel::IPerl::Kernel::Callback::REPL=HASH(0x2a70a18), Devel::IPerl::Kernel=HASH(0x1e58f80), Devel::IPerl::Message::ZMQ=HASH(0x572d5d0)) called at (eval 30) line 6\n\tDevel::IPerl::Kernel::Callback::REPL::execute(Devel::IPerl::Kernel::Callback::REPL=HASH(0x2a70a18), Devel::IPerl::Kernel=HASH(0x1e58f80), Devel::IPerl::Message::ZMQ=HASH(0x572d5d0)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel/Callback/REPL.pm line 156\n\tDevel::IPerl::Kernel::Callback::REPL::msg_execute_request(Devel::IPerl::Kernel::Callback::REPL=HASH(0x2a70a18), Devel::IPerl::Kernel=HASH(0x1e58f80), Devel::IPerl::Message::ZMQ=HASH(0x572d5d0), ZMQ::LibZMQ3::Socket=HASH(0x5705168)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel.pm line 245\n\tDevel::IPerl::Kernel::route_message(Devel::IPerl::Kernel=HASH(0x1e58f80), ARRAY(0x5705288), ZMQ::LibZMQ3::Socket=HASH(0x5705168)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel.pm line 215\n\tDevel::IPerl::Kernel::__ANON__(Net::Async::ZMQ::Socket=HASH(0x5705318)) called at /usr/local/lib/perl5/site_perl/5.32.1/IO/Async/Loop/Poll.pm line 172\n\tIO::Async::Loop::Poll::post_poll(IO::Async::Loop::Poll=HASH(0x5653b50)) called at /usr/local/lib/perl5/site_perl/5.32.1/IO/Async/Loop/Poll.pm line 292\n\tIO::Async::Loop::Poll::loop_once(IO::Async::Loop::Poll=HASH(0x5653b50), undef) called at /usr/local/lib/perl5/site_perl/5.32.1/IO/Async/Loop.pm line 538\n\tIO::Async::Loop::run(IO::Async::Loop::Poll=HASH(0x5653b50)) called at /usr/local/lib/perl5/site_perl/5.32.1/IO/Async/Loop.pm line 575\n\tIO::Async::Loop::loop_forever(IO::Async::Loop::Poll=HASH(0x5653b50)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel.pm line 225\n\tDevel::IPerl::Kernel::run(Devel::IPerl::Kernel=HASH(0x1e58f80)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl.pm line 14\n\tDevel::IPerl::main() called at -e line 1\n",
     "output_type": "error",
     "traceback": [
      "Undefined subroutine &main::net called at reply input line 16.\n at /usr/local/lib/perl5/site_perl/5.32.1/AI/MXNet/AutoGrad.pm line 408.\n\tAI::MXNet::AutoGrad::record(\"autograd\", CODE(0xb89ae50)) called at reply input line 18\n\tmain::train_epoch_ch3(CODE(0xb9a84e0), AI::MXNet::Gluon::Data::Loader::DataLoader=HASH(0xb99f6d0), AI::MXNet::Gluon::L2Loss=HASH(0xb9964f0), REF(0xba0d718)) called at reply input line 23\n\tmain::train(AI::MXNet::NDArray=HASH(0xb886910), AI::MXNet::NDArray=HASH(0xb9f7af0), AI::MXNet::NDArray=HASH(0x948d680), AI::MXNet::NDArray=HASH(0xb995c68)) called at reply input line 2\n\tEval::Closure::Sandbox_1288::__ANON__() called at /usr/local/lib/perl5/site_perl/5.32.1/Reply/Plugin/Defaults.pm line 71\n\tReply::Plugin::Defaults::execute(Reply::Plugin::Defaults=HASH(0x4ffc8b0), CODE(0xb886460), CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 217\n\tReply::_wrapped_plugin(Reply=HASH(0x5084558), ARRAY(0x572d7e0), \"execute\", CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 215\n\tReply::__ANON__(CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply/Plugin/IPerl.pm line 28\n\tReply::Plugin::IPerl::__ANON__() called at /usr/local/lib/perl5/site_perl/5.32.1/Capture/Tiny.pm line 382\n\teval {...} called at /usr/local/lib/perl5/site_perl/5.32.1/Capture/Tiny.pm line 382\n\tCapture::Tiny::_capture_tee(1, 1, 0, 0, CODE(0x2b15468)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply/Plugin/IPerl.pm line 29\n\tReply::Plugin::IPerl::execute(Reply::Plugin::IPerl=HASH(0x50b0fb0), CODE(0x572d468), CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 217\n\tReply::_wrapped_plugin(Reply=HASH(0x5084558), \"execute\", CODE(0xb892c70)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 174\n\tReply::_eval(Reply=HASH(0x5084558), \"\\x{a}#line 1 \\\"reply input\\\"\\x{a}# Pick the first two dimensions, i.e.,\"...) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 66\n\tReply::try {...} () called at /usr/local/lib/perl5/site_perl/5.32.1/Try/Tiny.pm line 102\n\teval {...} called at /usr/local/lib/perl5/site_perl/5.32.1/Try/Tiny.pm line 93\n\tTry::Tiny::try(CODE(0x57189c8), Try::Tiny::Catch=REF(0xb988cd0)) called at /usr/local/lib/perl5/site_perl/5.32.1/Reply.pm line 71\n\tReply::step(Reply=HASH(0x5084558), \"# Pick the first two dimensions, i.e., 1, x, from the polynom\"..., 0) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel/Backend/Reply.pm line 48\n\tDevel::IPerl::Kernel::Backend::Reply::__ANON__() called at /usr/local/lib/perl5/site_perl/5.32.1/Capture/Tiny.pm line 382\n\teval {...} called at /usr/local/lib/perl5/site_perl/5.32.1/Capture/Tiny.pm line 382\n\tCapture::Tiny::_capture_tee(1, 1, 0, 0, CODE(0xb86ead8)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel/Backend/Reply.pm line 49\n\tDevel::IPerl::Kernel::Backend::Reply::run_line(Devel::IPerl::Kernel::Backend::Reply=HASH(0x29aab28), \"# Pick the first two dimensions, i.e., 1, x, from the polynom\"...) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel/Callback/REPL.pm line 42\n\tDevel::IPerl::Kernel::Callback::REPL::execute(Devel::IPerl::Kernel::Callback::REPL=HASH(0x2a70a18), Devel::IPerl::Kernel=HASH(0x1e58f80), Devel::IPerl::Message::ZMQ=HASH(0x572d5d0)) called at (eval 30) line 6\n\tDevel::IPerl::Kernel::Callback::REPL::execute(Devel::IPerl::Kernel::Callback::REPL=HASH(0x2a70a18), Devel::IPerl::Kernel=HASH(0x1e58f80), Devel::IPerl::Message::ZMQ=HASH(0x572d5d0)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel/Callback/REPL.pm line 156\n\tDevel::IPerl::Kernel::Callback::REPL::msg_execute_request(Devel::IPerl::Kernel::Callback::REPL=HASH(0x2a70a18), Devel::IPerl::Kernel=HASH(0x1e58f80), Devel::IPerl::Message::ZMQ=HASH(0x572d5d0), ZMQ::LibZMQ3::Socket=HASH(0x5705168)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel.pm line 245\n\tDevel::IPerl::Kernel::route_message(Devel::IPerl::Kernel=HASH(0x1e58f80), ARRAY(0x5705288), ZMQ::LibZMQ3::Socket=HASH(0x5705168)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel.pm line 215\n\tDevel::IPerl::Kernel::__ANON__(Net::Async::ZMQ::Socket=HASH(0x5705318)) called at /usr/local/lib/perl5/site_perl/5.32.1/IO/Async/Loop/Poll.pm line 172\n\tIO::Async::Loop::Poll::post_poll(IO::Async::Loop::Poll=HASH(0x5653b50)) called at /usr/local/lib/perl5/site_perl/5.32.1/IO/Async/Loop/Poll.pm line 292\n\tIO::Async::Loop::Poll::loop_once(IO::Async::Loop::Poll=HASH(0x5653b50), undef) called at /usr/local/lib/perl5/site_perl/5.32.1/IO/Async/Loop.pm line 538\n\tIO::Async::Loop::run(IO::Async::Loop::Poll=HASH(0x5653b50)) called at /usr/local/lib/perl5/site_perl/5.32.1/IO/Async/Loop.pm line 575\n\tIO::Async::Loop::loop_forever(IO::Async::Loop::Poll=HASH(0x5653b50)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl/Kernel.pm line 225\n\tDevel::IPerl::Kernel::run(Devel::IPerl::Kernel=HASH(0x1e58f80)) called at /usr/local/lib/perl5/site_perl/5.32.1/Devel/IPerl.pm line 14\n\tDevel::IPerl::main() called at -e line 1\n"
     ]
    }
   ],
   "source": [
    "# Pick the first two dimensions, i.e., 1, x, from the polynomial features\n",
    "train(\n",
    "    getSubND({data=>$poly_features, row_start=>0, column_start=>0, row_end=>$n_train, column_end=>2}),\n",
    "    getSubND({data=>$poly_features, row_start=>$n_train+1, column_start=>0, row_end=>$poly_features->shape->[0], column_end=>2}),\n",
    "    getSubND({data=>$labels, row_start=>0, row_end=>$n_train}),\n",
    "    getSubND({data=>$labels, row_start=>$n_train+1, row_end=>$labels->shape->[0]})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e24b4",
   "metadata": {},
   "source": [
    "### Higher-Order Polynomial Function Fitting (Overfitting)\n",
    "Now let us try to train the model using a polynomial of too high degree. Here, there are insufficient\n",
    "data to learn that the higher-degree coefficients should have values close to zero. As a result, our\n",
    "overly-complex model is so susceptible that it is being influenced by noise in the training data.\n",
    "Though the training loss can be effectively reduced, the test loss is still much higher. It shows that\n",
    "the complex model overfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick all the dimensions from the polynomial features\n",
    "train(\n",
    "    getSubND({data=>$poly_features, row_start=>0, column_start=>0, row_end=>$n_train, column_end=>$poly_features->shape->[1]}),\n",
    "    getSubND({data=>$poly_features, row_start=>$n_train+1, column_start=>0, row_end=>$poly_features->shape->[0], column_end=>$poly_features->shape->[1]}),\n",
    "    getSubND({data=>$labels, row_start=>0, row_end=>$n_train}),\n",
    "    getSubND({data=>$labels, row_start=>$n_train+1, row_end=>$labels->shape->[0]}),\n",
    "    1500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bb25b",
   "metadata": {},
   "source": [
    "In the subsequent sections, we will continue to discuss overfitting problems and methods for dealing with them, such as weight decay and dropout.\n",
    "\n",
    "### Summary\n",
    "* Since the generalization error cannot be estimated based on the training error, simply minimizing the training error will not necessarily mean a reduction in the generalization error. <br> Machine learning models need to be careful to safeguard against overfitting so as to minimize the generalization error.\n",
    "* A validation set can be used for model selection, provided that it is not used too liberally.\n",
    "* Underfitting means that a model is not able to reduce the training error. When training error is much lower than validation error, there is overfitting.\n",
    "* We should choose an appropriately complex model and avoid using insufficient training samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
