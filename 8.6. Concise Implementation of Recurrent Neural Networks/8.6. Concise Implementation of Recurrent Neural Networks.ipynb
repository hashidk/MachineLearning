{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7110490",
   "metadata": {},
   "source": [
    "# 8.6. Concise Implementation of Recurrent Neural Networks\n",
    ":label:`sec_rnn-concise`\n",
    "\n",
    "While :numref:`sec_rnn_scratch` was instructive to see how RNNs are implemented,\n",
    "this is not convenient or fast.\n",
    "This section will show how to implement the same language model more efficiently\n",
    "using functions provided by high-level APIs\n",
    "of a deep learning framework.\n",
    "We begin as before by reading the time machine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ac7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "use strict;\n",
    "use warnings;\n",
    "use Data::Dump qw(dump);\n",
    "use AI::MXNet qw(mx);\n",
    "use d2l;\n",
    "use d2l::Vocab;\n",
    "use d2l::SeqDataLoader;\n",
    "use d2l::Timer;\n",
    "use d2l::Animator;\n",
    "use d2l::Accumulator;\n",
    "IPerl->load_plugin('Chart::Plotly'); # Jupyter\n",
    "#import Chart::Plotly 'show_plot'; # localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed1bd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CODE(0xc39ad70)Vocab=HASH(0xc41eb60)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my ($batch_size, $num_steps) = (32, 35);\n",
    "my ($train_iter, $vocab) = d2l->load_data_time_machine($batch_size, $num_steps);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ef7e9",
   "metadata": {},
   "source": [
    "## 8.6.1. Defining the Model\n",
    "\n",
    "High-level APIs provide implementations of recurrent neural networks.\n",
    "We construct the recurrent neural network layer `rnn_layer` with a single hidden layer and 256 hidden units.\n",
    "In fact, we have not even discussed yet what it means to have multiple layers---this will happen in :numref:`sec_deep_rnn`.\n",
    "For now, suffice it to say that multiple layers simply amount to the output of one layer of RNN being used as the input for the next layer of RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f451d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my $num_hiddens = 256;\n",
    "my $rnn_layer = mx->gluon->rnn->RNN($num_hiddens);\n",
    "$rnn_layer->initialize(mx->init->Xavier(), force_reinit => 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84672e52",
   "metadata": {},
   "source": [
    "Initializing the hidden state is straightforward.\n",
    "We invoke the member function `begin_state`.\n",
    "This returns a list (`state`)\n",
    "that contains\n",
    "an initial hidden state\n",
    "for each example in the minibatch,\n",
    "whose shape is\n",
    "(number of hidden layers, batch size, number of hidden units).\n",
    "For some models\n",
    "to be introduced later\n",
    "(e.g., long short-term memory),\n",
    "such a list also\n",
    "contains other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15940166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,  [1, 32, 256]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $state = $rnn_layer->begin_state($batch_size);\n",
    "print $#$state + 1, \",  \";\n",
    "print dump $state->[0]->shape;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3535e",
   "metadata": {},
   "source": [
    "With a hidden state and an input,\n",
    "we can compute the output with\n",
    "the updated hidden state.\n",
    "It should be emphasized that\n",
    "the \"output\" (`Y`) of `rnn_layer`\n",
    "does *not* involve computation of output layers:\n",
    "it refers to\n",
    "the hidden state at *each* time step,\n",
    "and they can be used as the input\n",
    "to the subsequent output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb422bf",
   "metadata": {},
   "source": [
    "Besides,\n",
    "the updated hidden state (`state_new`) returned by `rnn_layer`\n",
    "refers to the hidden state\n",
    "at the *last* time step of the minibatch.\n",
    "It can be used to initialize the\n",
    "hidden state for the next minibatch within an epoch\n",
    "in sequential partitioning.\n",
    "For multiple hidden layers,\n",
    "the hidden state of each layer will be stored\n",
    "in this variable (`state_new`).\n",
    "For some models\n",
    "to be introduced later\n",
    "(e.g., long short-term memory),\n",
    "this variable also\n",
    "contains other information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ef9437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 32, 256], 1, [1, 32, 256]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $X = mx->nd->random->uniform(shape => [$num_steps, $batch_size, $vocab->len]);\n",
    "my ($Y, $state_new) = @{$rnn_layer->forward($X, $state)};\n",
    "print dump $Y->shape;\n",
    "print \", \", $#$state_new + 1;\n",
    "print \", \", dump $state_new->[0]->shape;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f9b98",
   "metadata": {},
   "source": [
    "Similar to :numref:`sec_rnn_scratch`,\n",
    "we define an `RNNModel` class\n",
    "for a complete RNN model.\n",
    "Note that `rnn_layer` only contains the hidden recurrent layers, we need to create a separate output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fc8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "package RNNModel {\n",
    "    use base qw(AI::MXNet::Gluon::Block);\n",
    "    use strict; \n",
    "    use warnings;\n",
    "    use Data::Dump qw(dump);\n",
    "    use AI::MXNet qw(mx);\n",
    "    use AI::MXNet::Gluon qw(gluon);\n",
    "    \n",
    "    #The RNN model.\n",
    "    \n",
    "    sub new {\n",
    "        my ($class,  $rnn_layer, $vocab_size, %kwargs) = (shift, @_);\n",
    "        my $self = $class->SUPER::new(%kwargs);\n",
    "        \n",
    "        $self->{rnn} = $rnn_layer;\n",
    "        $self->{vocab_size} = $vocab_size;\n",
    "        $self->{dense} = mx->gluon->nn->Dense($vocab_size);\n",
    "        \n",
    "        foreach my $name('rnn', 'dense'){\n",
    "            if( defined $name){\n",
    "                $self->register_child($self->{$name});\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return bless($self, $class);\n",
    "    }\n",
    "    \n",
    "    sub forward{\n",
    "        my ($self, $inputs, $state) = @_;\n",
    "        my $X =  mx->nd->one_hot($inputs->T, $self->{vocab_size});\n",
    "        my $Y;\n",
    "        ($Y, $state) = @{$self->{rnn}->forward($X, $state)};\n",
    "        # The fully-connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        my $output = $self->{dense}->($Y->reshape([-1, $Y->shape->[-1]]));\n",
    "        return ($output, $state);\n",
    "    }\n",
    "    \n",
    "    sub begin_state{\n",
    "        my ($self, $args) = @_;\n",
    "        return $self->{rnn}->begin_state($args);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28dab4",
   "metadata": {},
   "source": [
    "## 8.6.2. Training and Predicting\n",
    "\n",
    "Before training the model, let us make a prediction with the a model that has random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c192b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time travellerzzzzzzzzzz"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $device = d2l->try_gpu();\n",
    "my $net = RNNModel->new($rnn_layer, $vocab->len);\n",
    "$net->initialize(mx->init->Xavier(), force_reinit => 1, ctx => $device);\n",
    "d2l->predict_ch8('time traveller', 10, $net, $vocab, $device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268a160",
   "metadata": {},
   "source": [
    "As is quite obvious, this model does not work at all. Next, we call `train_ch8` with the same hyperparameters defined in :numref:`sec_rnn_scratch` and train our model with high-level APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40442c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity 1.2, 3053.9 tokens/sec on cpu(0)\n",
      "time traveller returnsiv time travellingv in the gollen the form\n",
      "travellergon age it und line and thatline there is the futu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>\n",
       "//# sourceURL=iperl-devel-plugin-chart-plotly.js\n",
       "            $('#Plotly').each(function(i, e) { $(e).attr('id', 'plotly') });\n",
       "\n",
       "            if (!window.Plotly) {\n",
       "                requirejs.config({\n",
       "                  paths: {\n",
       "                    plotly: ['https://cdn.plot.ly/plotly-latest.min']},\n",
       "                });\n",
       "                window.Plotly = {\n",
       "react : function (div, data, layout, config){\n",
       "                    require(['plotly'], function(plotly) {\n",
       "                      window.Plotly=plotly;\n",
       "Plotly.react(div, data, layout, config);                    });\n",
       "                  }\n",
       "                }\n",
       "            }\n",
       "</script>\n",
       "<div id=\"14b3e96d-b9d7-11ed-bc22-86affd31e0d6\"></div>\n",
       "\n",
       "<script>\n",
       "Plotly.react(document.getElementById('14b3e96d-b9d7-11ed-bc22-86affd31e0d6'),[{\"x\":[10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500],\"type\":\"scatter\",\"line\":{\"width\":3,\"dash\":\"dot\"},\"mode\":\"lines\",\"name\":\"train\",\"y\":[10.0339095449292,7.50681514785387,6.00227936806178,4.85585657487249,3.90782689684888,3.21900666146879,2.77705412571676,2.37700602712994,2.10669672684697,1.89746304854961,1.7509867642719,1.61667901895999,1.54868918625532,1.46550157392696,1.41666073317218,1.38025997428088,1.35235234048609,1.31171694767798,1.26756611814778,1.27210149870849,1.25854281728963,1.25710810180852,1.22756798463791,1.22367493706351,1.21407479971888,1.20405788987161,1.20020198404297,1.19950517531397,1.19022228144404,1.18416210966475,1.18393561524507,1.18085101757866,1.18285693162168,1.17987143183078,1.17185601820175,1.18915660195075,1.15456413576691,1.17528183135804,1.16212880379577,1.16285722987879,1.17612138556654,1.15618657677144,1.16648733769566,1.16867822944916,1.15532596642169,1.14960553682102,1.16296988741216,1.15202680406536,1.13215429780181,1.15155608330329]}] ,{\"hight\":600,\"yaxis\":{\"title\":\"perplexity\"},\"width\":800,\"xaxis\":{\"range\":[10,\"500\"],\"title\":\"epoch\"}} );\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my ($num_epochs, $lr) = (500, 1);\n",
    "my ($model_file_name, $is_train, $animator) = ('GoogLeNet.mdl', 1);\n",
    "if ($is_train){\n",
    "  $animator = d2l->train_ch8($net, $train_iter, $vocab, $lr, $num_epochs, $device);\n",
    "  $net->save_parameters($model_file_name);\n",
    "  $animator->plot;\n",
    "\n",
    "}else{\n",
    "  $net->load_parameters($model_file_name);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144ea4a",
   "metadata": {},
   "source": [
    "Compared with the last section, this model achieves comparable perplexity,\n",
    "albeit within a shorter period of time, due to the code being more optimized by\n",
    "high-level APIs of the deep learning framework.\n",
    "\n",
    "\n",
    "## 8.6.3.Summary\n",
    "\n",
    "* High-level APIs of the deep learning framework provides an implementation of the RNN layer.\n",
    "* The RNN layer of high-level APIs returns an output and an updated hidden state, where the output does not involve output layer computation.\n",
    "* Using high-level APIs leads to faster RNN training than using its implementation from scratch.\n",
    "\n",
    "## 8.6.4. Exercises\n",
    "\n",
    "1. Can you make the RNN model overfit using the high-level APIs?\n",
    "1. What happens if you increase the number of hidden layers in the RNN model? Can you make the model work?\n",
    "1. Implement the autoregressive model of :numref:`sec_sequence` using an RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
